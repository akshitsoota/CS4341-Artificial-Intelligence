GROUP MEMBERS: Ankit Kumar, Akshit Soota, Dan Alfred
TEAM NAME: ASKAgents

HOW TO COMPILE:
To compile our program, first navigate into src/student/cs4341/project2. Then, run the following command:
javac evaluation/*.java fileinterface/*.java game/*.java *.java
(In sum, compile all of the java files in each of the source folders together)

The compiled .class files should now be present in their respective directories.
Additionally, our program was compiled and tested with Java 8, but should also compile on Java 7.

HOW TO RUN:
To run our program, navigate to src/ and run the following command:
java student.cs4341.project2.Main

The program will print out "Initialized game for ASKAgents" to indicate that it is ready.

UTILITY FUNCTION:
Our program utilizes a simple utility function to indicate wins or losses when evaluating. We assigned a value of
+10000 for wins, and -10000 for losses. Both of these values were selected because they are higher than any
value the evaluation function could return, indicating a terminal state with certainty.

EVALUATION FUNCTION:
To evaluate non-terminal states of the board, we have implemented an algorithm that considers the state of each
player in the game. Our evaluation function loops through the board and counts, for the given move, how many
N in-a-row it and the enemy have. We factor in 4, 3, and 2 in-a-row, and assign prime coefficients for each to
avoid overlap. The enemy's coefficients are negative to indicate that an improvement in this state is worse for
the player. However, the magnitude of our coefficients lead the AI to play offensively, valuing 4 in-a-rows of
our own before blocking potential enemy 3 in-a-rows from becoming 4 in-a-rows. 

EXPANSION HEURISTICS:
We implemented heuristics to keep the minimax algorithm from expanding every possible state. Our heuristics
only allow for expansion of game states where the planned move is adjacent to an existing move. This is because
a move anywhere else on the board would have no immediate impact on the game, and because the opponent would
not likely pick a move that is not adjacent to the current playing area.

TIME-LIMIT STRATEGY:
To ensure the time limit is met, we utilized a separate thread to conduct computations on the possible game states
while maintaining a global variable with the best move so far. Once the thread timer has reached 8 seconds (2
seconds allowed as buffer for finalizing computations and writing to file), we take the best possible move and
submit it to the referee.

RESULTS AND TESTING:
To test our program, we launched two instances with different group names and had them play against each other.
We observed our AI blocking enemy 3 in-a-rows and attempting to create its own. Additionally, we used a modified
referee which printed to board to confirm what moves were being made, and compared the moves made by each other
to the moves that we would have made ourselves.

STRENGTHS AND WEAKNESSES:
We found during play that our AI had several strengths. First, our AI utilizes several heuristics for
performance optimizations. For example, we discard any game states where the given move is not connected to an
existing stone, as these moves are irrelevant to the next turn. Additionally, our utility function is set to
halt immediately once a terminal state is discovered. These two heuristics allow our program to begin exploring
Depth 3 in our game tree before the timer is up. A second strength is the inclusion of open and half-open logic.
This heuristic allows for consideration if the AI or enemy has three stones in a row with no stones or borders
blocking the outsides, it is deemed "open" and can result in a terminal state if the AI does not take action
immediately. Conversely, "half-open" states where the N in-a-row have only one side open are less urgent and
may not need to be addressed immediately.

We also found several weaknesses in our algorithms. Firstly, the open and half-open logic was only applied to
rows and columns, not diagonals, which could result in undetected wins by a smarter opponent. Additionally,
our AI assumes that the opponent always makes the optimal move. For example, if the AI reaches a state where
the opponent has used a fork and all moves (assuming optimal opponent) would result in a loss, the AI plays the
first move it expands. This is not the best solution, as a suboptimal opponent might not notice one of the terminal
states and would allow our AI to keep playing.

DISCUSSION:
Given the timeframe for this project, we think the heuristics and evaluation function developed are sufficient.
Our heuristics are modeled around good decisions in the game, such as determining which boards to expand and
what plays can be made to get a 3 or 4 in-a-row. This heuristic in particular takes minimal time to execute
compared to the time saved by removing irrelevant board states. Our open/half-open heuristic also allows for
additional playmaking ability for our AI based on actual strategy from Gomoku.

We think our evaluation function for nodes that cannot be expanded is satisfactory as well. The basis of this
function was to maximize our 4, 3, and 2 in-a-rows while minimizing that of the opponent, which models how
humans play Gomoku. Although our algorithm is inefficient because it goes through each column, row, and diagonal
to determine how many in-a-row exist, this time is reduced by pruning the boards that can be evaluated. We were
also considering an evaluation function that maintained an array with several indexes for a given coordinate on
the board, indicating how many rows, columns, and diagonals that coordinate was a part of. This array would be
updated each move, and since it is stored, the number of calculations done would be greatly reduced. However,
due to memory limitations and the size of the board, it seemed more feasible to redo the calculations rather than
store.